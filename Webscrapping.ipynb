{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo as bibliotecas\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def scoresfixtures(link,ids):\n",
    "    '''\n",
    "    Description: This Class picks all the games that had in one season and combinate all links to one especific list\n",
    "    \n",
    "    Inputs:\n",
    "        - link: The link of the main page that have all season games desired.\n",
    "        - ids: The ID of the championship table\n",
    "        \n",
    "    Outputs:\n",
    "        - especific list that has all the links os all matches of the season\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    req = requests.get(link)\n",
    "    if req.status_code == 200:\n",
    "        content = req.content\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    tb = soup.find(id=ids)\n",
    "\n",
    "    s1= []\n",
    "    s2= []\n",
    "    for i in tb.find_all(\"a\"):\n",
    "            s1.append(str(i))\n",
    "            s2.append(str(i.get_text('href')))\n",
    "\n",
    "\n",
    "    # Calling DataFrame constructor after zipping \n",
    "    # both lists, with columns specified \n",
    "    di = pd.DataFrame(list(zip(s1, s2)), \n",
    "                   columns =['Codes', 'ID']) \n",
    "\n",
    "    s4=[]\n",
    "    for i in di[\"Codes\"]:\n",
    "        i = i.replace('<a href=\"','')\n",
    "        i = i.replace('</a>','')\n",
    "        s4.append(str(i))\n",
    "\n",
    "\n",
    "    s5 = []\n",
    "\n",
    "    for i in di['Codes']:\n",
    "        if \"matches\" in i:\n",
    "            s5.append(str(i))\n",
    "        else:\n",
    "            s5.append(0)\n",
    "\n",
    "    s6 = []\n",
    "    for i in di[\"Codes\"]:\n",
    "        if '<a href=\"/en/squads/' in i:\n",
    "            i = i.replace('<a href=\"/en/squads/','')\n",
    "            i = i[0:8]\n",
    "            s6.append(str(i))\n",
    "        else:\n",
    "            s6.append(0)        \n",
    "\n",
    "    # Calling DataFrame constructor after zipping \n",
    "    # both lists, with columns specified \n",
    "    da = pd.DataFrame(list(zip(s1, s2,s4,s5,s6)), \n",
    "                   columns =['CODES', 'ID','URL_FINAL','PARTIDAS_2019',\"TEAM_CODE\"])        \n",
    "\n",
    "    s9 = []\n",
    "    for i in da[\"URL_FINAL\"]:\n",
    "        if 'Match Report' in i:\n",
    "            s9.append(str(i))\n",
    "        else:\n",
    "            pass\n",
    "    return s9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscoresfixtures\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://fbref.com/en/comps/24/2019/stats/2019-Serie-A-Stats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstats_standard\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [1], line 32\u001b[0m, in \u001b[0;36mscoresfixtures\u001b[0;34m(link, ids)\u001b[0m\n\u001b[1;32m     30\u001b[0m s1\u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m s2\u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     33\u001b[0m         s1\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(i))\n\u001b[1;32m     34\u001b[0m         s2\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "scoresfixtures(\"https://fbref.com/en/comps/24/2019/stats/2019-Serie-A-Stats\",\"stats_standard\")\n",
    "#sched_2019_24_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def planilhas(url):\n",
    "    '''\n",
    "    Description: This function goes to de URL of the match and treat all data in order to append it in one single Dataframe.\n",
    "    \n",
    "    Input:\n",
    "        - url: Url of the html page\n",
    "        \n",
    "    Output:\n",
    "        - Dataframe treated from the match saved on my machine excel file\n",
    "    \n",
    "    '''\n",
    "    #make the request\n",
    "    pg = 'https://fbref.com'\n",
    "    url_pg = pg+ url\n",
    "    req = requests.get(url_pg)\n",
    "    if req.status_code == 200:\n",
    "        content = req.content\n",
    "    #accessing data from site\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    table_geral = soup.find_all(class_ = \"table_outer_container\")\n",
    "    table_time_1 = table_geral[0]\n",
    "    table_time_2 = table_geral[2]\n",
    "    table_time_3 = soup.find(class_='venuetime')\n",
    "\n",
    "    #collecting data\n",
    "\n",
    "\n",
    "    table_torcida = soup.find_all('div',class_ = \"scorebox_meta\")\n",
    "    oi_torcida = str(table_torcida)\n",
    "\n",
    "\n",
    "    #treating data\n",
    "    toby = oi_torcida.split('<small>')\n",
    "    torcida = str(toby[2])\n",
    "    torcida = torcida.split('</small>')\n",
    "    torcida = str(torcida[0])\n",
    "    estadio = toby[4]\n",
    "    estadio = estadio.split('</small>')\n",
    "    estadio = str(estadio[0])    \n",
    "\n",
    "\n",
    "    \n",
    "    #collecting data\n",
    "    data = table_time_3.get('data-venue-date')        \n",
    "\n",
    "    \n",
    "    #treating data\n",
    "    nome = str(soup.title)\n",
    "    nome = nome.replace(\" \",\"_\")\n",
    "    nome = nome.replace(\"<title>\",\"\")\n",
    "    nome = nome.replace(\".\",\"\")\n",
    "    nome_final = nome.split(\"Report\")[0]\n",
    "\n",
    "\n",
    "    #treating data\n",
    "\n",
    "    nome_final = nome_final.split(\"_Match\")\n",
    "    nome_final = nome_final[0]    \n",
    "\n",
    "\n",
    "    # STR transform and reading tables\n",
    "    table_str_1 = str(table_time_1)\n",
    "    table_str_2 = str(table_time_2)\n",
    "    df_1 = pd.read_html(table_str_1)[0]\n",
    "    df_2 = pd.read_html(table_str_2)[0]\n",
    "\n",
    "    #treating data\n",
    "\n",
    "    time = str(nome_final)\n",
    "    time = time.replace(\"_\",\" \")\n",
    "    time = time.split(\" vs \")\n",
    "    time_1 = str(time[0])\n",
    "    time_2 = str(time[1])\n",
    "\n",
    "    #Dtframe transforming\n",
    "    df_1 = pd.DataFrame(df_1)\n",
    "    df_1.columns = df_1.columns.droplevel()\n",
    "    df_1['Time'] = str(time_1)\n",
    "    df_1['Time_Adversario'] = str(time_2)\n",
    "    df_1['Confronto'] = str(nome_final)\n",
    "    df_1['Data'] = str(data)\n",
    "    df_1['Estadio'] = str(estadio)\n",
    "    df_1['Torcida'] = torcida    \n",
    "\n",
    "\n",
    "    df_2 = pd.DataFrame(df_2)\n",
    "    df_2.columns = df_2.columns.droplevel()\n",
    "    df_2['Time'] = str(time_2)\n",
    "    df_2['Time_Adversario'] = str(time_1)\n",
    "    df_2['Confronto'] = str(nome_final)\n",
    "    df_2['Data'] = str(data)\n",
    "    df_2['Estadio'] = str(estadio)\n",
    "    df_2['Torcida'] = torcida\n",
    "\n",
    "\n",
    "    #APPENDING Dataframes\n",
    "    df_3 = df_1.append(df_2)\n",
    "    \n",
    "    #save excel \n",
    "    writer = pd.ExcelWriter('cool.xlsx')\n",
    "    df_3.to_excel(writer,\"Estatisticas\")\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamento(nome): \n",
    "    '''\n",
    "    Description: This function goes through all files of the directiory and joins all them in one single dataframe saving in\n",
    "    excel sheet.\n",
    "    \n",
    "    Input:\n",
    "        - Nome: Name that you want for your excel sheet\n",
    "        \n",
    "    Output:\n",
    "        - Dataframe of all games save as excel sheet\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    entries = os.listdir()\n",
    "\n",
    "    base = {}\n",
    "\n",
    "    base = pd.DataFrame(base)\n",
    "\n",
    "\n",
    "    for i in entries:\n",
    "        if \"xlsx\" in i:\n",
    "            base = base.append(pd.read_excel(i))\n",
    "            base = base.drop_duplicates()\n",
    "\n",
    "        #escrevendo em excelfile\n",
    "        else:\n",
    "            pass\n",
    "    writer = pd.ExcelWriter(\"data.xlsx\")\n",
    "    base.to_excel(writer,nome)\n",
    "    writer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoresfixtures(\"https://fbref.com/en/comps/24/2019/schedule/2019-Serie-A-Scores-and-Fixtures\",\"sched_2019_24_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m scoresfixtures(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://fbref.com/en/comps/24/2019/schedule/2019-Serie-A-Scores-and-Fixtures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msched_2019_24_1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mplanilhas\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start))\n",
      "Cell \u001b[0;32mIn [58], line 22\u001b[0m, in \u001b[0;36mplanilhas\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     19\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m table_geral \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_outer_container\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m table_time_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtable_geral\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m table_time_2 \u001b[38;5;241m=\u001b[39m table_geral[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     24\u001b[0m table_time_3 \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvenuetime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for url in scoresfixtures(\"https://fbref.com/en/comps/24/2019/schedule/2019-Serie-A-Scores-and-Fixtures\",\"sched_2019_24_1\"):\n",
    "    planilhas(url)\n",
    "    \n",
    "\n",
    "print('Duration: {} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tratamento(\"Brasileirao\")\n",
    "\n",
    "print('Duration: {} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
